{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b648de44",
   "metadata": {},
   "source": [
    "# Instagram Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://dogtime.com/dog-breeds/profiles'\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36',\n",
    "}\n",
    "\n",
    "page = requests.get(URL)\n",
    "soup = bs(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a440f74",
   "metadata": {},
   "source": [
    "## Get All The Dog Breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by letter - gbl\n",
    "dogs_breeds = []\n",
    "for gbl in soup.find_all('div',{'class': 'article-crumbs'}):\n",
    "    for breed in gbl.find_all('a', {'class': 'list-item-title'}):\n",
    "        dogs_breeds.append(breed.get_text().replace(\"-\", \" \").upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4d66b",
   "metadata": {},
   "source": [
    "## Insert To DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d69f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "breeds_path = 'C:/Users/Asus/Data Science Project/Stansford Dataset/images/Images'\n",
    "breeds_stansford = []\n",
    "for filename in os.listdir(breeds_path):\n",
    "        word = filename[filename.find('-'):][1:].replace(\"_\", \" \")\n",
    "        word = word.replace(\"-\", \" \")\n",
    "        word= word[:1].upper() + word[1:]\n",
    "        breeds_stansford.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91410079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'breed':breeds_stansford}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6604a",
   "metadata": {},
   "source": [
    "## Sepcific Breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_breeds = []\n",
    "\n",
    "path = os.getcwd()\n",
    "my_file = open(os.path.join(path ,\"Breeds_File.txt\"), \"r\")\n",
    "content = my_file.read()\n",
    "content_list = content.split(\"\\n\")\n",
    "my_file.close()\n",
    "\n",
    "for breed in content_list:\n",
    "        word = breed.replace(\"_\", \" \").replace(\"-\", \" \").replace(\" \", \"\")\n",
    "        word= word[:1].upper() + word[1:]\n",
    "        final_breeds.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffd751",
   "metadata": {},
   "source": [
    "# Crawling Instagram With Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69294476",
   "metadata": {},
   "source": [
    "## Log in Instagram account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import wget\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca91360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set env var\n",
    "os.environ['insta_username'] = \"****\"\n",
    "os.environ['insta_password'] = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome('C:/Users/Asus/Documents/WebDriver/chromedriver.exe')\n",
    "s = Service('C:/Users/Asus/Documents/WebDriver/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.instagram.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait for a maximum of 10 seconds for an element\n",
    "username = WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "password = WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "username.clear()\n",
    "password.clear()\n",
    "\n",
    "username.send_keys(os.environ['insta_username'])\n",
    "password.send_keys(os.environ['insta_password'])\n",
    "time.sleep(7)\n",
    "log_in = WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "time.sleep(5)\n",
    "not_now = WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Not Now')]\"))).click()\n",
    "#not now 2 for notification settings\n",
    "time.sleep(5)\n",
    "not_now2 = WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Not Now')]\"))).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3d366",
   "metadata": {},
   "source": [
    "## Breeds by Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856a4ef",
   "metadata": {},
   "source": [
    "### Base Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(query):\n",
    "    if not os.path.exists(os.getcwd() + '/' + query):\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, query)\n",
    "        os.mkdir(path)\n",
    "        print(query + \" Folder Created\")\n",
    "    else:\n",
    "        print(query + \" Folder Already Exist\")\n",
    "        \n",
    "\n",
    "def save_images(images, query):\n",
    "    counter = 0\n",
    "    path = os.getcwd()\n",
    "    path = os.path.join(path, query)\n",
    "    for img in images:\n",
    "        save_as = os.path.join(path ,query + str(counter) +'.jpg')\n",
    "        wget.download(img, save_as)\n",
    "        counter += 1\n",
    "    \n",
    "    print(\"%s Images Saved! | Total of: %s Images\" %(query,counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bd4bf",
   "metadata": {},
   "source": [
    "### Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "for breed in final_breeds:\n",
    "    \n",
    "    img_urls = []\n",
    "    query = breed.replace(\" \", \"\")\n",
    "    page = driver.get(\"https://www.instagram.com/explore/tags/\" + query)\n",
    "\n",
    "    #scroll\n",
    "    element = wait.until(EC.element_to_be_clickable((By.TAG_NAME, 'img')))\n",
    "    scrolldown = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\n",
    "    #window.scrollTo(0, document.body.scrollHeight);\n",
    "    count_scroll=0 \n",
    "    match=False\n",
    "    count_opens=0\n",
    "    \n",
    "    while(match==False):\n",
    "        element = wait.until(EC.element_to_be_clickable((By.TAG_NAME, 'img')))\n",
    "        time.sleep(random.randint(2,4))\n",
    "        images = driver.find_elements(By.TAG_NAME,'img')\n",
    "        time.sleep(random.randint(2,4))\n",
    "        images = [image.get_attribute('src') for image in images]\n",
    "        img_urls.extend(images)\n",
    "        #Remove Duplicates Of Img URLs\n",
    "        img_urls = list( dict.fromkeys(img_urls))\n",
    "        images.clear()\n",
    "        print(\"Images: %s | Counter: %s\" % (len(img_urls), count_scroll))\n",
    "        time.sleep(random.randint(3,4))\n",
    "        \n",
    "        last_count = scrolldown\n",
    "        scrolldown = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\n",
    "        \n",
    "        if(last_count==scrolldown):\n",
    "            time.sleep(random.randint(2,4))     \n",
    "            imgclick = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#react-root > section > main > article > div:nth-child(3) > div > div:nth-child(11) > div:nth-child(1)\"))).click()            \n",
    "            time.sleep(random.randint(7,10))                                \n",
    "            not_now = driver.find_element_by_xpath('//div[@class=\"QBdPU \"]/*[name()=\"svg\"][@aria-label=\"Close\"]').click()\n",
    "            \n",
    "            \n",
    "        if(random.randint(0, 6)==6):\n",
    "            time.sleep(random.randint(2,4))     \n",
    "            imgclick = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#react-root > section > main > article > div:nth-child(3) > div > div:nth-child(11) > div:nth-child(1)\"))).click()            \n",
    "            time.sleep(random.randint(7,10))                                \n",
    "            not_now = driver.find_element_by_xpath('//div[@class=\"QBdPU \"]/*[name()=\"svg\"][@aria-label=\"Close\"]').click()\n",
    "       \n",
    "       \n",
    "        \n",
    "        if ( (count_scroll==835) or (len(img_urls)>10000)):\n",
    "            match=True\n",
    "        count_scroll+=1\n",
    "    \n",
    "    #create path if dosent exsist\n",
    "    create_folder(query)\n",
    "    \n",
    "    #save the images\n",
    "    save_images(img_urls, query)\n",
    "    \n",
    "    #sleep dealy between get request\n",
    "    time.sleep(random.randint(40,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccaca4",
   "metadata": {},
   "source": [
    "### In this project all the data came from scrapping and the crawling is used for educational purpose only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
