{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54880e75",
   "metadata": {},
   "source": [
    "# Google Images Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4348c",
   "metadata": {},
   "source": [
    "### Setup the dogs breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_breeds = []\n",
    "\n",
    "path = os.getcwd()\n",
    "my_file = open(os.path.join(path ,\"Breeds_File.txt\"), \"r\")\n",
    "content = my_file.read()\n",
    "content_list = content.split(\"\\n\")\n",
    "my_file.close()\n",
    "\n",
    "for breed in content_list:\n",
    "        word = breed.replace(\"_\", \" \").replace(\"-\", \" \").replace(\" \", \"\")\n",
    "        word= word[:1].upper() + word[1:]\n",
    "        final_breeds.append(word)\n",
    "        \n",
    "print(final_breeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6018e3f7",
   "metadata": {},
   "source": [
    "## Creating The Selenium Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e756ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "#chrome_options.add_argument('headless')\n",
    "#driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "s = Service('C:/Users/Asus/Documents/WebDriver/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.google.com/')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f83d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_URL = \"https://www.google.com/search?tbm=isch&q=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68150d19",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa52fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down(driver):\n",
    "    img_urls = []\n",
    "    try:\n",
    "        # Get scroll height.\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            images = driver.find_elements(By.TAG_NAME,'img')\n",
    "            time.sleep(random.randint(2,4))\n",
    "            images = [image.get_attribute('src') for image in images]\n",
    "            img_urls.extend(images)\n",
    "            #Remove Duplicates Of Img URLs\n",
    "            img_urls = list( dict.fromkeys(img_urls))\n",
    "            images.clear()\n",
    "            print(\"Images: %s\" % (len(img_urls)))\n",
    "            # Scroll down to the bottom.\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(random.randint(3,6))\n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                time.sleep(random.randint(5,15))\n",
    "                try:\n",
    "                    load_more_clickable= WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[value='Show more results']\"))).click()\n",
    "                except:\n",
    "                    break\n",
    "            last_height = new_height\n",
    "            \n",
    "    except ElementNotInteractableException:\n",
    "        raise Exception(\"Exception -> end of page\")\n",
    "    \n",
    "    return img_urls\n",
    "        \n",
    "        \n",
    "def download_image(url, folder_name, num):\n",
    "    # write image to file\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code==200:\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, folder_name)\n",
    "        with open(os.path.join(path, folder_name + str(num)+\".jpg\"), 'wb') as file:\n",
    "            file.write(reponse.content)\n",
    "            \n",
    "def create_folder(query):\n",
    "    if not os.path.exists(os.getcwd() + '/' + query):\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, query)\n",
    "        os.mkdir(path)\n",
    "        print(query + \" Folder Created\")\n",
    "    else:\n",
    "        print(query + \" Folder Already Exist\")\n",
    "        \n",
    "def save_images(images, query):\n",
    "    counter = 0\n",
    "    path = os.getcwd()\n",
    "    path = os.path.join(path, query)\n",
    "    for img in images:\n",
    "        save_as = os.path.join(path ,query + str(counter) +'.jpg')\n",
    "        wget.download(img, save_as)\n",
    "        counter += 1\n",
    "    \n",
    "    print(\"%s Images Saved! | Total of: %s Images\" %(query,counter))\n",
    "    \n",
    "\n",
    "def decode_b64(img_data, num, query):\n",
    "    path = os.getcwd()\n",
    "    path = os.path.join(path, query)\n",
    "    \n",
    "    # Separate the metadata from the image data\n",
    "    head, data = img_data.split(',', 1)\n",
    "\n",
    "    # Get the file extension (gif, jpeg, png)\n",
    "    file_ext = head.split(';')[0].split('/')[1]\n",
    "\n",
    "    # Decode the image data\n",
    "    plain_data = base64.b64decode(data)\n",
    "    \n",
    "    #create full image path\n",
    "    save_as = os.path.join(path ,query + str(num) +'.' + file_ext)\n",
    "    # Write the image to a file\n",
    "    with open(save_as, 'wb') as f:\n",
    "        f.write(plain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5caaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "for breed in final_breeds:\n",
    "    \n",
    "    print(\"Search images for: \" + breed)\n",
    "    \n",
    "    img_urls_b64 = []\n",
    "    img_urls = []\n",
    "    query = breed.replace(\" \", \"\")\n",
    "    page = driver.get(search_URL + query)\n",
    "\n",
    "    img_urls_b64 = scroll_down(driver)\n",
    "    print(\"Images: %s\" % (len(img_urls_b64)))\n",
    "    \n",
    "    #create path if dosent exsist\n",
    "    create_folder(query)\n",
    "    \n",
    "    #Convert data-uri's to img files\n",
    "    count = 0\n",
    "    for img_b64 in img_urls_b64:\n",
    "        count += 1\n",
    "        decode_b64(img_b64, count, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "for breed in final_breeds:\n",
    "    \n",
    "    print(\"Search images for: \" + breed)\n",
    "    \n",
    "    img_urls = []\n",
    "    query = breed.replace(\" \", \"\")\n",
    "    page = driver.get(search_URL + query)\n",
    "\n",
    "    img_urls = scroll_down(driver)\n",
    "    \n",
    "    page_html = driver.page_source\n",
    "    pageSoup = bs(page_html, 'html.parser')\n",
    "    containers = pageSoup.findAll('div', {'class':\"isv-r PNCib MSM1fd BUooTd\"} )\n",
    "\n",
    "    print(\"Number of images for %s: %s\"%(query,len(containers)))  \n",
    "    \n",
    "    len_containers = len(containers)\n",
    "    create_folder(query)\n",
    "    \n",
    "    for i in range(1, len_containers+1):\n",
    "        if i % 25 == 0:\n",
    "            continue\n",
    "\n",
    "        xPath = \"\"\"//*[@id=\"islrg\"]/div[1]/div[%s]\"\"\"%(i)\n",
    "\n",
    "        previewImageXPath = \"\"\"//*[@id=\"islrg\"]/div[1]/div[%s]/a[1]/div[1]/img\"\"\"%(i)\n",
    "        \n",
    "        try:\n",
    "            previewImageElement = driver.find_element(By.XPATH, previewImageXPath)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        previewImageURL = previewImageElement.get_attribute(\"src\")\n",
    "            \n",
    "        driver.find_element(By.XPATH, xPath).click()\n",
    "        timeStarted = time.time()\n",
    "\n",
    "        while True:\n",
    "            imageElement = driver.find_element(By.XPATH, \"\"\"//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div[1]/a/img\"\"\")\n",
    "            imageURL= imageElement.get_attribute('src')\n",
    "\n",
    "            if imageURL != previewImageURL:\n",
    "                #print(\"actual URL\", imageURL)\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                #making a timeout if the full res image can't be loaded\n",
    "                currentTime = time.time()\n",
    "\n",
    "                if currentTime - timeStarted > 10:\n",
    "                    print(\"Timeout! Will download a lower resolution image and move onto the next one\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        #Downloading image\n",
    "        try:\n",
    "            download_image(imageURL, query, i)\n",
    "            print(\"Downloaded element %s out of %s total. URL: %s\" % (i, len_containers + 1, imageURL))\n",
    "        except:\n",
    "            print(\"Couldn't download an image %s, continuing downloading the next one\"%(i))\n",
    "    \n",
    "    time.sleep(random.randint(20,45))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e9c1d",
   "metadata": {},
   "source": [
    "### In this project all the data came from scrapping and the crawling is used for educational purpose only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
