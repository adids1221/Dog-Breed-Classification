{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0999cd",
   "metadata": {},
   "source": [
    "# Pond5 Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084abb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f86815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878a838",
   "metadata": {},
   "source": [
    "## Import breeds from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29baee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_breeds = []\n",
    "\n",
    "path = os.getcwd()\n",
    "my_file = open(os.path.join(path ,\"Breed_pond5.txt\"), \"r\")\n",
    "content = my_file.read()\n",
    "content_list = content.split(\"\\n\")\n",
    "my_file.close()\n",
    "\n",
    "for breed in content_list:\n",
    "        word = breed.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "        word= word[:1].upper() + word[1:]\n",
    "        final_breeds.append(word)\n",
    "        \n",
    "print(final_breeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef11ac",
   "metadata": {},
   "source": [
    "## Selenium driver setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "s = Service('C:/Users/Asus/Documents/WebDriver/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.pond5.com/')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1be1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_URL = \"https://www.pond5.com/\"\n",
    "page_url = \"&pp=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f152eef",
   "metadata": {},
   "source": [
    "## Pond5 Crawling/Scraping  Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad415926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(query):\n",
    "    if not os.path.exists(os.getcwd() + '/' + query):\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, query)\n",
    "        os.mkdir(path)\n",
    "        print(query + \" Folder Created\")\n",
    "    else:\n",
    "        print(query + \" Folder Already Exist\")\n",
    "        \n",
    "def save_images(images, query):\n",
    "    counter = 0\n",
    "    path = os.getcwd()\n",
    "    path = os.path.join(path, query)\n",
    "    for img in images:\n",
    "        save_as = os.path.join(path ,query + str(counter) +'.jpg')\n",
    "        try:\n",
    "            wget.download(img, save_as)\n",
    "        except Exception as e:\n",
    "            print(\"Could not download file %s\"%(img))\n",
    "            print(e)\n",
    "            continue\n",
    "        counter += 1\n",
    "    \n",
    "    print(\"%s Images Saved! | Total of: %s Images\" %(query,counter))\n",
    "    \n",
    "def save_single_images(img, query):\n",
    "    counter = 0\n",
    "    path = os.getcwd()\n",
    "    path = os.path.join(path, query)\n",
    "    save_as = os.path.join(path ,query + str(counter) +'.jpg')\n",
    "    print(img)\n",
    "    wget.download(img, save_as)\n",
    "    \n",
    "    print(\"%s Images Saved!\" %(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down(driver):\n",
    "    img_urls = []\n",
    "    try:\n",
    "        # Get scroll height.\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            time.sleep(random.randint(1,3))\n",
    "            page_source = driver.page_source\n",
    "            soup = bs(page_source, 'html.parser')\n",
    "            img_div = soup.find('div',{'class':'SearchResultsV3-mosaic js-searchResultsList'})\n",
    "            links = img_div.find_all('img')\n",
    "            for link in links:\n",
    "                try:\n",
    "                    img_urls.append(link['src'])\n",
    "                except:\n",
    "                    img_urls.append(link['data-src'])\n",
    "            \n",
    "            time.sleep(random.randint(2,4))\n",
    "\n",
    "            #Remove Duplicates Of Img URLs\n",
    "            img_urls = list( dict.fromkeys(img_urls))\n",
    "            # Scroll down to the bottom.\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(random.randint(3,6))\n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight/2\")\n",
    "            if new_height == last_height:\n",
    "                print(\"End Of Page..\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "    except ElementNotInteractableException:\n",
    "        raise Exception(\"Exception -> end of page\")\n",
    "        \n",
    "    return img_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8197e1",
   "metadata": {},
   "source": [
    "## Use The Automation For Each Breed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5f37b",
   "metadata": {},
   "source": [
    "#### In this automation we used Selenium and BeautifulSoup combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "for breed in final_breeds:\n",
    "    \n",
    "    print(\"Search images for: \" + breed)\n",
    "    img_urls = []\n",
    "    img_download_links = []\n",
    "    query = breed\n",
    "    search_URL = f\"https://www.pond5.com/search?kw={breed}&media=photo\"\n",
    "    page = driver.get(search_URL + page_url + \"1\") \n",
    "\n",
    "    time.sleep(random.randint(3,6))\n",
    "    page_source = driver.page_source\n",
    "    soup = bs(page_source, 'html.parser')\n",
    "    total_pages = soup.find('div',{'class':'Arrange-sizeFit Arrange-right js-paginationCount'})\n",
    "    print(\"Total pages to run: %s\"%(total_pages.get_text()))\n",
    "    \n",
    "    num_page = int(total_pages.get_text())\n",
    "    count=0\n",
    "    for i in range(1, num_page+1):\n",
    "        img_urls = scroll_down(driver)\n",
    "        time.sleep(random.randint(2,4))\n",
    "        for link in img_urls:\n",
    "            try:\n",
    "                img_download_links.append(link)\n",
    "            except:\n",
    "                continue        \n",
    "        print(\"Total Images: %s\"%(len(img_download_links)))\n",
    "        if(len(img_download_links) > 5000):\n",
    "            break\n",
    "        page = driver.get(search_URL + page_url + str(i+1))\n",
    "    \n",
    "    folder_name = query + \"Pond5\"\n",
    "    #create path if dosent exsist\n",
    "    create_folder(folder_name)\n",
    "    \n",
    "    #save the images\n",
    "    save_images(img_download_links,folder_name)\n",
    "    print(\"Donwload for %s finished, Total of %s images!\"%(breed,len(img_download_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2a698",
   "metadata": {},
   "source": [
    "### In this project all the data came from scrapping and the crawling is used for educational purpose only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
