{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0dc4e9",
   "metadata": {},
   "source": [
    "# Flicker Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f614d",
   "metadata": {},
   "source": [
    "## Import breeds from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51568b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_breeds = []\n",
    "\n",
    "path = os.getcwd()\n",
    "my_file = open(os.path.join(path ,\"Breeds_File.txt\"), \"r\")\n",
    "content = my_file.read()\n",
    "content_list = content.split(\"\\n\")\n",
    "my_file.close()\n",
    "\n",
    "for breed in content_list:\n",
    "        word = breed.replace(\"_\", \" \").replace(\"-\", \" \").replace(\" \", \"\")\n",
    "        word= word[:1].upper() + word[1:]\n",
    "        final_breeds.append(word)\n",
    "        \n",
    "print(final_breeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c51abc",
   "metadata": {},
   "source": [
    "## Selenium driver setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "s = Service('C:/Users/Asus/Documents/WebDriver/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.flickr.com/')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_URL = \"https://www.flickr.com/search/?text=\"\n",
    "main_URL = \"https://www.flickr.com/\"\n",
    "download_URl = \"sizes/l/\"\n",
    "show_all_extension = \"&view_all=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4def428",
   "metadata": {},
   "source": [
    "## Scroll Down Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7552a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down(driver):\n",
    "    img_urls = []\n",
    "    try:\n",
    "        # Get scroll height.\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            time.sleep(random.randint(1,3))\n",
    "            page_source = driver.page_source\n",
    "            soup = bs(page_source, 'html.parser')\n",
    "            links = soup.find_all('a',{'class':'overlay'}, href=True)\n",
    "            for link in links:\n",
    "                img_urls.append(\"https://www.flickr.com\"+link['href'])\n",
    "\n",
    "            time.sleep(random.randint(2,4))\n",
    "\n",
    "            #Remove Duplicates Of Img URLs\n",
    "            img_urls = list( dict.fromkeys(img_urls))\n",
    "            print(\"Images: %s\" % (len(img_urls)))\n",
    "            if(len(img_urls)>15000):\n",
    "                break\n",
    "            # Scroll down to the bottom.\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(random.randint(3,6))\n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                time.sleep(random.randint(7,12)) \n",
    "                try:\n",
    "                    load_more_clickable = WebDriverWait(driver,60).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Load more results')]\"))).click()\n",
    "                    print(\"Clicked Load More...\")\n",
    "                except:\n",
    "                    print(\"End Of Page..\")\n",
    "                    break\n",
    "            last_height = new_height\n",
    "            \n",
    "    except ElementNotInteractableException:\n",
    "        raise Exception(\"Exception -> end of page\")\n",
    "    \n",
    "    return img_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c496de7",
   "metadata": {},
   "source": [
    "## Flicker Crawling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99056eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flicker_image_crawel(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(random.randint(3,6))\n",
    "    page_source = driver.page_source\n",
    "    soup = bs(page_source, 'html.parser')\n",
    "    download_nav = soup.find('div',{'class':'view photo-engagement-view'})\n",
    "    download_icon = download_nav.find('div',{'class':'engagement-item download'})\n",
    "    download_link = download_icon.find('a', href=True)\n",
    "    img_url = flicker_image_downloader(main_URL+download_link['href'])\n",
    "    return img_url\n",
    "\n",
    "def flicker_image_downloader(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(random.randint(3,6))\n",
    "    page_source = driver.page_source\n",
    "    soup = bs(page_source, 'html.parser')\n",
    "    try:\n",
    "        img_div = soup.find('div',{'id':'allsizes-photo'})\n",
    "        img = img_div.find('img')['src']\n",
    "    except:\n",
    "        pass\n",
    "    return img\n",
    "\n",
    "def flicker_image_download_link(url):\n",
    "    driver.get(url+download_URl)\n",
    "    time.sleep(random.randint(2,4))\n",
    "    page_source = driver.page_source\n",
    "    soup = bs(page_source, 'html.parser')\n",
    "    try:\n",
    "        img_div = soup.find('div',{'id':'allsizes-photo'})\n",
    "        img = img_div.find('img')['src']\n",
    "    except:\n",
    "        print(\"Cant get image link!\")\n",
    "        pass\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55823b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, folder_name, num):\n",
    "    # write image to file\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code==200:\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, folder_name)\n",
    "        with open(os.path.join(path, folder_name + str(num)+\".jpg\"), 'wb') as file:\n",
    "            file.write(reponse.content)\n",
    "            \n",
    "def create_folder(query):\n",
    "    if not os.path.exists(os.getcwd() + '/' + query):\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, query)\n",
    "        os.mkdir(path)\n",
    "        print(query + \" Folder Created\")\n",
    "    else:\n",
    "        print(query + \" Folder Already Exist\")\n",
    "        \n",
    "def save_images(images, query):\n",
    "    counter = 0\n",
    "    path = os.getcwd()\n",
    "    path = os.path.join(path, query)\n",
    "    for img in images:\n",
    "        if(img):\n",
    "            save_as = os.path.join(path ,query + str(counter) +'.jpg')\n",
    "            wget.download(img, save_as)\n",
    "            counter += 1\n",
    "        else:\n",
    "            print(\"Cant retrive image will continue...\")\n",
    "            pass\n",
    "    \n",
    "    print(\"%s Images Saved! | Total of: %s Images\" %(query,counter))\n",
    "    \n",
    "def save_single_images(img, query):\n",
    "    if(img):\n",
    "        counter = 0\n",
    "        path = os.getcwd()\n",
    "        path = os.path.join(path, query)\n",
    "        save_as = os.path.join(path ,query + str(counter) +'.jpg')\n",
    "        print(img)\n",
    "        wget.download(img, save_as)\n",
    "    else:\n",
    "        print(\"Cant retrive image will continu...\")\n",
    "    \n",
    "    print(\"%s Images Saved!\" %(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff97295",
   "metadata": {},
   "source": [
    "## Use The Automation For Each Breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "for breed in final_breeds:\n",
    "    \n",
    "    print(\"Search images for: \" + breed)\n",
    "    img_urls = []\n",
    "    img_download_links = []\n",
    "    query = breed.replace(\" \", \"\")\n",
    "    page = driver.get(search_URL + query + show_all_extension) \n",
    "\n",
    "    img_urls = scroll_down(driver)\n",
    "    print(\"Images: %s\" % (len(img_urls)))\n",
    "    print(img_urls)\n",
    "    \n",
    "    count = 0\n",
    "    for img in img_urls:\n",
    "        img_download_links.append(flicker_image_download_link(img))\n",
    "    \n",
    "    print(\"Len:%s\"%(len(img_download_links)))\n",
    "    \n",
    "    #create path if dosent exsist\n",
    "    create_folder(query)\n",
    "    \n",
    "    #save the images\n",
    "    save_images(img_download_links,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de6c82",
   "metadata": {},
   "source": [
    "### In this project all the data came from scrapping and the crawling is used for educational purpose only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
